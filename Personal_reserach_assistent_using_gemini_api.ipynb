{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install openai requests pathlib dataclasses argparse"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 654
        },
        "id": "e0BZ3ik-2JcN",
        "outputId": "a24db1b4-7c3f-4c1a-a0b7-1645b31c129f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.98.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Collecting pathlib\n",
            "  Downloading pathlib-1.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting dataclasses\n",
            "  Downloading dataclasses-0.6-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting argparse\n",
            "  Downloading argparse-1.4.0-py2.py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
            "Downloading pathlib-1.0.1-py3-none-any.whl (14 kB)\n",
            "Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
            "Downloading argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
            "Installing collected packages: pathlib, dataclasses, argparse\n",
            "Successfully installed argparse-1.4.0 dataclasses-0.6 pathlib-1.0.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "argparse",
                  "dataclasses",
                  "pathlib"
                ]
              },
              "id": "9f73a93a9d684b6f974657dee4d768bb"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QAQPykBU20_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Personal Research Assistant - Google Colab Version with Gemini API\n",
        "# Fetches academic papers, summarizes key findings, and organizes notes using Google's Gemini API\n",
        "\n",
        "# ========================================\n",
        "# INSTALLATION AND SETUP\n",
        "# ========================================\n",
        "\n",
        "# Install required packages\n",
        "!pip install google-generativeai requests xmltodict python-dotenv -q\n",
        "\n",
        "# Import libraries\n",
        "import os\n",
        "import json\n",
        "import requests\n",
        "import time\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Optional, Tuple\n",
        "import xml.etree.ElementTree as ET\n",
        "from urllib.parse import quote\n",
        "import google.generativeai as genai\n",
        "from dataclasses import dataclass\n",
        "import logging\n",
        "from google.colab import files\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "# Configure logging for Colab\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# ========================================\n",
        "# DATA CLASSES AND CONFIGURATIONS\n",
        "# ========================================\n",
        "\n",
        "@dataclass\n",
        "class Paper:\n",
        "    \"\"\"Data class for academic papers\"\"\"\n",
        "    title: str\n",
        "    authors: List[str]\n",
        "    abstract: str\n",
        "    url: str\n",
        "    published_date: str\n",
        "    source: str\n",
        "    doi: Optional[str] = None\n",
        "    categories: List[str] = None\n",
        "\n",
        "class ColabConfig:\n",
        "    \"\"\"Configuration class for Colab environment\"\"\"\n",
        "    def __init__(self):\n",
        "        self.base_path = Path(\"/content/research_projects\")\n",
        "        self.base_path.mkdir(exist_ok=True)\n",
        "\n",
        "    def setup_gemini_key(self):\n",
        "        \"\"\"Interactive setup for Gemini API key\"\"\"\n",
        "        from getpass import getpass\n",
        "\n",
        "        print(\"Google Gemini API Key Setup\")\n",
        "        print(\"=\" * 40)\n",
        "        print(\"Get your FREE API key at: https://aistudio.google.com/app/apikey\")\n",
        "        print()\n",
        "\n",
        "        # Check if key already exists in environment\n",
        "        if os.getenv('GOOGLE_API_KEY'):\n",
        "            print(\"Gemini API key already configured!\")\n",
        "            return os.getenv('GOOGLE_API_KEY')\n",
        "\n",
        "        # Interactive key input\n",
        "        api_key = getpass(\"Enter your Google Gemini API key (hidden input): \")\n",
        "\n",
        "        if not api_key or len(api_key) < 20:\n",
        "            raise ValueError(\"Invalid Gemini API key format. Please check your key.\")\n",
        "\n",
        "        # Set environment variable\n",
        "        os.environ['GOOGLE_API_KEY'] = api_key\n",
        "        genai.configure(api_key=api_key)\n",
        "        print(\"API key configured successfully!\")\n",
        "        return api_key\n",
        "\n",
        "# ========================================\n",
        "# PAPER FETCHING CLASSES\n",
        "# ========================================\n",
        "\n",
        "class PaperFetcher:\n",
        "    \"\"\"Handles fetching papers from various academic sources\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.session = requests.Session()\n",
        "        self.session.headers.update({\n",
        "            'User-Agent': 'Research Assistant Bot 1.0 (Educational Purpose)'\n",
        "        })\n",
        "\n",
        "    def fetch_arxiv_papers(self, query: str, max_results: int = 10) -> List[Paper]:\n",
        "        \"\"\"Fetch papers from ArXiv\"\"\"\n",
        "        print(f\"Searching ArXiv for: '{query}'\")\n",
        "\n",
        "        base_url = \"http://export.arxiv.org/api/query\"\n",
        "        params = {\n",
        "            'search_query': f\"all:{query}\",\n",
        "            'start': 0,\n",
        "            'max_results': max_results,\n",
        "            'sortBy': 'submittedDate',\n",
        "            'sortOrder': 'descending'\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = self.session.get(base_url, params=params, timeout=30)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            papers = []\n",
        "            root = ET.fromstring(response.content)\n",
        "\n",
        "            for entry in root.findall('{http://www.w3.org/2005/Atom}entry'):\n",
        "                title = entry.find('{http://www.w3.org/2005/Atom}title').text.strip()\n",
        "                abstract = entry.find('{http://www.w3.org/2005/Atom}summary').text.strip()\n",
        "\n",
        "                authors = []\n",
        "                for author in entry.findall('{http://www.w3.org/2005/Atom}author'):\n",
        "                    name = author.find('{http://www.w3.org/2005/Atom}name').text\n",
        "                    authors.append(name)\n",
        "\n",
        "                published = entry.find('{http://www.w3.org/2005/Atom}published').text\n",
        "                url = entry.find('{http://www.w3.org/2005/Atom}id').text\n",
        "\n",
        "                # Extract categories\n",
        "                categories = []\n",
        "                for category in entry.findall('{http://www.w3.org/2005/Atom}category'):\n",
        "                    categories.append(category.get('term'))\n",
        "\n",
        "                paper = Paper(\n",
        "                    title=title,\n",
        "                    authors=authors,\n",
        "                    abstract=abstract,\n",
        "                    url=url,\n",
        "                    published_date=published,\n",
        "                    source=\"ArXiv\",\n",
        "                    categories=categories\n",
        "                )\n",
        "                papers.append(paper)\n",
        "\n",
        "            print(f\"Found {len(papers)} papers from ArXiv\")\n",
        "            return papers\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching ArXiv papers: {e}\")\n",
        "            return []\n",
        "\n",
        "    def fetch_semantic_scholar_papers(self, query: str, max_results: int = 10) -> List[Paper]:\n",
        "        \"\"\"Fetch papers from Semantic Scholar\"\"\"\n",
        "        print(f\"Searching Semantic Scholar for: '{query}'\")\n",
        "\n",
        "        base_url = \"https://api.semanticscholar.org/graph/v1/paper/search\"\n",
        "        params = {\n",
        "            'query': query,\n",
        "            'limit': max_results,\n",
        "            'fields': 'title,authors,abstract,url,publicationDate,externalIds'\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = self.session.get(base_url, params=params, timeout=30)\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "\n",
        "            papers = []\n",
        "            for item in data.get('data', []):\n",
        "                if not item.get('abstract'):\n",
        "                    continue\n",
        "\n",
        "                authors = [author.get('name', '') for author in item.get('authors', [])]\n",
        "                external_ids = item.get('externalIds', {})\n",
        "                doi = external_ids.get('DOI')\n",
        "\n",
        "                paper = Paper(\n",
        "                    title=item.get('title', ''),\n",
        "                    authors=authors,\n",
        "                    abstract=item.get('abstract', ''),\n",
        "                    url=item.get('url', ''),\n",
        "                    published_date=item.get('publicationDate', ''),\n",
        "                    source=\"Semantic Scholar\",\n",
        "                    doi=doi\n",
        "                )\n",
        "                papers.append(paper)\n",
        "\n",
        "            print(f\"Found {len(papers)} papers from Semantic Scholar\")\n",
        "            return papers\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching Semantic Scholar papers: {e}\")\n",
        "            return []\n",
        "\n",
        "    def fetch_pubmed_papers(self, query: str, max_results: int = 10) -> List[Paper]:\n",
        "        \"\"\"Fetch papers from PubMed (bonus source)\"\"\"\n",
        "        print(f\"Searching PubMed for: '{query}'\")\n",
        "\n",
        "        # First, search for paper IDs\n",
        "        search_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\"\n",
        "        search_params = {\n",
        "            'db': 'pubmed',\n",
        "            'term': query,\n",
        "            'retmax': max_results,\n",
        "            'retmode': 'json',\n",
        "            'sort': 'pub_date'\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            search_response = self.session.get(search_url, params=search_params, timeout=30)\n",
        "            search_response.raise_for_status()\n",
        "            search_data = search_response.json()\n",
        "\n",
        "            paper_ids = search_data.get('esearchresult', {}).get('idlist', [])\n",
        "\n",
        "            if not paper_ids:\n",
        "                print(\"No papers found from PubMed\")\n",
        "                return []\n",
        "\n",
        "            # Fetch paper details\n",
        "            fetch_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi\"\n",
        "            fetch_params = {\n",
        "                'db': 'pubmed',\n",
        "                'id': ','.join(paper_ids),\n",
        "                'retmode': 'xml'\n",
        "            }\n",
        "\n",
        "            fetch_response = self.session.get(fetch_url, params=fetch_params, timeout=30)\n",
        "            fetch_response.raise_for_status()\n",
        "\n",
        "            papers = []\n",
        "            root = ET.fromstring(fetch_response.content)\n",
        "\n",
        "            for article in root.findall('.//PubmedArticle'):\n",
        "                try:\n",
        "                    # Extract title\n",
        "                    title_elem = article.find('.//ArticleTitle')\n",
        "                    title = title_elem.text if title_elem is not None else \"No title\"\n",
        "\n",
        "                    # Extract abstract\n",
        "                    abstract_elem = article.find('.//AbstractText')\n",
        "                    abstract = abstract_elem.text if abstract_elem is not None else \"No abstract available\"\n",
        "\n",
        "                    # Extract authors\n",
        "                    authors = []\n",
        "                    for author in article.findall('.//Author'):\n",
        "                        lastname = author.find('LastName')\n",
        "                        firstname = author.find('ForeName')\n",
        "                        if lastname is not None and firstname is not None:\n",
        "                            authors.append(f\"{firstname.text} {lastname.text}\")\n",
        "\n",
        "                    # Extract publication date\n",
        "                    pub_date = article.find('.//PubDate/Year')\n",
        "                    published_date = pub_date.text if pub_date is not None else \"Unknown\"\n",
        "\n",
        "                    # Extract PMID for URL\n",
        "                    pmid_elem = article.find('.//PMID')\n",
        "                    pmid = pmid_elem.text if pmid_elem is not None else \"\"\n",
        "                    url = f\"https://pubmed.ncbi.nlm.nih.gov/{pmid}/\" if pmid else \"\"\n",
        "\n",
        "                    paper = Paper(\n",
        "                        title=title,\n",
        "                        authors=authors,\n",
        "                        abstract=abstract,\n",
        "                        url=url,\n",
        "                        published_date=published_date,\n",
        "                        source=\"PubMed\"\n",
        "                    )\n",
        "                    papers.append(paper)\n",
        "\n",
        "                except Exception as e:\n",
        "                    continue  # Skip problematic papers\n",
        "\n",
        "            print(f\"Found {len(papers)} papers from PubMed\")\n",
        "            return papers\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching PubMed papers: {e}\")\n",
        "            return []\n",
        "\n",
        "# ========================================\n",
        "# SUMMARIZATION CLASS WITH GEMINI\n",
        "# ========================================\n",
        "\n",
        "class GeminiResearchSummarizer:\n",
        "    \"\"\"Handles summarization using Google's Gemini API\"\"\"\n",
        "\n",
        "    def __init__(self, api_key: str, model_name: str = \"gemini-1.5-flash\"):\n",
        "        genai.configure(api_key=api_key)\n",
        "        self.model = genai.GenerativeModel(model_name)\n",
        "        self.model_name = model_name\n",
        "\n",
        "    def summarize_paper(self, paper: Paper) -> Dict[str, str]:\n",
        "        \"\"\"Generate a comprehensive summary of a paper\"\"\"\n",
        "        print(f\"Summarizing: {paper.title[:60]}...\")\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "        Please analyze this academic paper and provide a structured summary in JSON format:\n",
        "\n",
        "        Title: {paper.title}\n",
        "        Authors: {', '.join(paper.authors)}\n",
        "        Abstract: {paper.abstract}\n",
        "        Source: {paper.source}\n",
        "\n",
        "        Provide your analysis in the following JSON format:\n",
        "        {{\n",
        "            \"key_findings\": \"Main discoveries and results (2-3 sentences)\",\n",
        "            \"methodology\": \"Research methods used (1-2 sentences)\",\n",
        "            \"significance\": \"Why this research matters (1-2 sentences)\",\n",
        "            \"limitations\": \"Study limitations or future work needed (1-2 sentences)\",\n",
        "            \"keywords\": [\"keyword1\", \"keyword2\", \"keyword3\", \"keyword4\", \"keyword5\"],\n",
        "            \"summary\": \"Comprehensive overview (4-5 sentences)\",\n",
        "            \"practical_applications\": \"Real-world applications (1-2 sentences)\"\n",
        "        }}\n",
        "\n",
        "        Be concise but informative. Focus on the most important aspects. Return only valid JSON.\n",
        "        \"\"\"\n",
        "\n",
        "        try:\n",
        "            response = self.model.generate_content(prompt)\n",
        "            content = response.text.strip()\n",
        "\n",
        "            # Try to parse JSON response\n",
        "            try:\n",
        "                # Clean the response to extract JSON\n",
        "                if '```json' in content:\n",
        "                    content = content.split('```json')[1].split('```')[0]\n",
        "                elif '```' in content:\n",
        "                    content = content.split('```')[1].split('```')[0]\n",
        "\n",
        "                return json.loads(content)\n",
        "            except json.JSONDecodeError:\n",
        "                # Fallback summary\n",
        "                return {\n",
        "                    \"key_findings\": \"Could not parse structured analysis\",\n",
        "                    \"methodology\": \"Not specified\",\n",
        "                    \"significance\": \"Not specified\",\n",
        "                    \"limitations\": \"Not specified\",\n",
        "                    \"keywords\": [\"analysis\", \"research\", \"academic\"],\n",
        "                    \"summary\": content[:500] + \"...\" if len(content) > 500 else content,\n",
        "                    \"practical_applications\": \"Not specified\"\n",
        "                }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error summarizing paper: {e}\")\n",
        "            return {\n",
        "                \"key_findings\": \"Summary generation failed\",\n",
        "                \"methodology\": \"Not available\",\n",
        "                \"significance\": \"Not available\",\n",
        "                \"limitations\": \"Not available\",\n",
        "                \"keywords\": [\"error\"],\n",
        "                \"summary\": paper.abstract[:500] + \"...\" if len(paper.abstract) > 500 else paper.abstract,\n",
        "                \"practical_applications\": \"Not available\"\n",
        "            }\n",
        "\n",
        "    def generate_literature_review(self, papers: List[Paper], summaries: List[Dict], query: str) -> str:\n",
        "        \"\"\"Generate a comprehensive literature review\"\"\"\n",
        "        print(\"Generating literature review...\")\n",
        "\n",
        "        paper_summaries = []\n",
        "        for paper, summary in zip(papers, summaries):\n",
        "            paper_summaries.append({\n",
        "                \"title\": paper.title,\n",
        "                \"authors\": paper.authors[:3],  # Limit authors for brevity\n",
        "                \"key_findings\": summary.get(\"key_findings\", \"\"),\n",
        "                \"significance\": summary.get(\"significance\", \"\"),\n",
        "                \"keywords\": summary.get(\"keywords\", [])[:5]\n",
        "            })\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "        Write a comprehensive literature review based on the following research papers about \"{query}\":\n",
        "\n",
        "        Number of papers analyzed: {len(papers)}\n",
        "\n",
        "        Paper summaries:\n",
        "        {json.dumps(paper_summaries, indent=2)}\n",
        "\n",
        "        Your literature review should include:\n",
        "\n",
        "        1. **Introduction** - Overview of the research area and its importance\n",
        "        2. **Current State of Research** - Key themes and findings across studies\n",
        "        3. **Methodological Approaches** - Common research methods used\n",
        "        4. **Key Findings and Trends** - Major discoveries and patterns\n",
        "        5. **Gaps and Limitations** - Areas needing further research\n",
        "        6. **Future Directions** - Suggestions for upcoming research\n",
        "        7. **Conclusion** - Summary of the field's current status\n",
        "\n",
        "        Write in an academic style, approximately 1000-1200 words. Use clear headings and make connections between different studies.\n",
        "        Cite papers by their titles when referencing specific findings.\n",
        "        \"\"\"\n",
        "\n",
        "        try:\n",
        "            response = self.model.generate_content(prompt)\n",
        "            return response.text.strip()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating literature review: {e}\")\n",
        "            return f\"# Literature Review: {query}\\n\\nLiterature review generation encountered an error. Please review individual paper summaries for insights.\"\n",
        "\n",
        "    def generate_research_insights(self, papers: List[Paper], summaries: List[Dict], query: str) -> str:\n",
        "        \"\"\"Generate additional research insights and trends\"\"\"\n",
        "        print(\"Generating research insights...\")\n",
        "\n",
        "        keywords_all = []\n",
        "        for summary in summaries:\n",
        "            keywords_all.extend(summary.get(\"keywords\", []))\n",
        "\n",
        "        # Count keyword frequency\n",
        "        keyword_counts = {}\n",
        "        for keyword in keywords_all:\n",
        "            keyword_counts[keyword] = keyword_counts.get(keyword, 0) + 1\n",
        "\n",
        "        top_keywords = sorted(keyword_counts.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "        Based on the research about \"{query}\", provide insights about trends and patterns:\n",
        "\n",
        "        Total papers analyzed: {len(papers)}\n",
        "        Top keywords found: {[kw[0] for kw in top_keywords[:5]]}\n",
        "\n",
        "        Generate insights covering:\n",
        "\n",
        "        1. **Emerging Trends** - What new directions are researchers exploring?\n",
        "        2. **Research Gaps** - What important questions remain unanswered?\n",
        "        3. **Methodological Evolution** - How are research methods changing?\n",
        "        4. **Cross-disciplinary Connections** - How does this field connect to others?\n",
        "        5. **Future Predictions** - Where might this field be heading?\n",
        "\n",
        "        Keep each section to 2-3 paragraphs. Be analytical and forward-thinking.\n",
        "        \"\"\"\n",
        "\n",
        "        try:\n",
        "            response = self.model.generate_content(prompt)\n",
        "            return response.text.strip()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" Error generating insights: {e}\")\n",
        "            return \"Research insights generation failed.\"\n",
        "\n",
        "# ========================================\n",
        "# FILE ORGANIZATION CLASS\n",
        "# ========================================\n",
        "\n",
        "class ColabResearchOrganizer:\n",
        "    \"\"\"Handles file organization optimized for Colab\"\"\"\n",
        "\n",
        "    def __init__(self, base_path: str = \"/content/research_projects\"):\n",
        "        self.base_path = Path(base_path)\n",
        "        self.base_path.mkdir(exist_ok=True)\n",
        "\n",
        "    def create_project_folder(self, project_name: str) -> Path:\n",
        "        \"\"\"Create a new research project folder\"\"\"\n",
        "        project_path = self.base_path / self.sanitize_filename(project_name)\n",
        "        project_path.mkdir(exist_ok=True)\n",
        "\n",
        "        # Create subfolders\n",
        "        (project_path / \"papers\").mkdir(exist_ok=True)\n",
        "        (project_path / \"summaries\").mkdir(exist_ok=True)\n",
        "        (project_path / \"notes\").mkdir(exist_ok=True)\n",
        "        (project_path / \"data\").mkdir(exist_ok=True)\n",
        "        (project_path / \"insights\").mkdir(exist_ok=True)\n",
        "\n",
        "        return project_path\n",
        "\n",
        "    def sanitize_filename(self, filename: str) -> str:\n",
        "        \"\"\"Sanitize filename for cross-platform compatibility\"\"\"\n",
        "        invalid_chars = '<>:\"/\\\\|?*'\n",
        "        for char in invalid_chars:\n",
        "            filename = filename.replace(char, '_')\n",
        "        return filename[:50]\n",
        "\n",
        "    def save_paper_data(self, project_path: Path, papers: List[Paper]):\n",
        "        \"\"\"Save raw paper data\"\"\"\n",
        "        papers_data = []\n",
        "        for paper in papers:\n",
        "            papers_data.append({\n",
        "                \"title\": paper.title,\n",
        "                \"authors\": paper.authors,\n",
        "                \"abstract\": paper.abstract,\n",
        "                \"url\": paper.url,\n",
        "                \"published_date\": paper.published_date,\n",
        "                \"source\": paper.source,\n",
        "                \"doi\": paper.doi,\n",
        "                \"categories\": paper.categories\n",
        "            })\n",
        "\n",
        "        with open(project_path / \"data\" / \"papers_data.json\", 'w', encoding='utf-8') as f:\n",
        "            json.dump(papers_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        print(f\"Saved {len(papers)} papers to papers_data.json\")\n",
        "\n",
        "    def save_summaries(self, project_path: Path, papers: List[Paper], summaries: List[Dict]):\n",
        "        \"\"\"Save paper summaries\"\"\"\n",
        "        all_summaries = []\n",
        "\n",
        "        for paper, summary in zip(papers, summaries):\n",
        "            filename = self.sanitize_filename(paper.title) + \".json\"\n",
        "\n",
        "            summary_data = {\n",
        "                \"title\": paper.title,\n",
        "                \"authors\": paper.authors,\n",
        "                \"url\": paper.url,\n",
        "                \"source\": paper.source,\n",
        "                \"published_date\": paper.published_date,\n",
        "                \"summary\": summary,\n",
        "                \"generated_on\": datetime.now().isoformat()\n",
        "            }\n",
        "\n",
        "            # Save individual summary\n",
        "            with open(project_path / \"summaries\" / filename, 'w', encoding='utf-8') as f:\n",
        "                json.dump(summary_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "            all_summaries.append(summary_data)\n",
        "\n",
        "        # Save consolidated summaries\n",
        "        with open(project_path / \"data\" / \"all_summaries.json\", 'w', encoding='utf-8') as f:\n",
        "            json.dump(all_summaries, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        print(f\"Saved {len(summaries)} summaries\")\n",
        "\n",
        "    def save_literature_review(self, project_path: Path, review: str, query: str):\n",
        "        \"\"\"Save literature review\"\"\"\n",
        "        review_content = f\"\"\"# Literature Review: {query}\n",
        "\n",
        "**Generated on:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "**Research Query:** {query}\n",
        "**AI Model:** Google Gemini\n",
        "\n",
        "---\n",
        "\n",
        "{review}\n",
        "\n",
        "---\n",
        "\n",
        "*This literature review was automatically generated by the Personal Research Assistant using Google's Gemini AI.*\n",
        "\"\"\"\n",
        "\n",
        "        with open(project_path / \"notes\" / \"literature_review.md\", 'w', encoding='utf-8') as f:\n",
        "            f.write(review_content)\n",
        "\n",
        "        print(\"Literature review saved\")\n",
        "\n",
        "    def save_research_insights(self, project_path: Path, insights: str, query: str):\n",
        "        \"\"\"Save research insights\"\"\"\n",
        "        insights_content = f\"\"\"# Research Insights: {query}\n",
        "\n",
        "**Generated on:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "**Research Query:** {query}\n",
        "**AI Model:** Google Gemini\n",
        "\n",
        "---\n",
        "\n",
        "{insights}\n",
        "\n",
        "---\n",
        "\n",
        "*These insights were automatically generated by the Personal Research Assistant using Google's Gemini AI.*\n",
        "\"\"\"\n",
        "\n",
        "        with open(project_path / \"insights\" / \"research_insights.md\", 'w', encoding='utf-8') as f:\n",
        "            f.write(insights_content)\n",
        "\n",
        "        print(\"Research insights saved\")\n",
        "\n",
        "    def create_project_summary(self, project_path: Path, query: str, papers: List[Paper]):\n",
        "        \"\"\"Create a comprehensive project summary\"\"\"\n",
        "        sources = {}\n",
        "        for paper in papers:\n",
        "            sources[paper.source] = sources.get(paper.source, 0) + 1\n",
        "\n",
        "        summary_content = f\"\"\"# Research Project: {query}\n",
        "\n",
        "**Created:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "**Query:** \"{query}\"\n",
        "**Papers Found:** {len(papers)}\n",
        "**AI Model:** Google Gemini (Free API)\n",
        "\n",
        "##  Paper Sources\n",
        "\"\"\"\n",
        "\n",
        "        for source, count in sources.items():\n",
        "            summary_content += f\"- **{source}:** {count} papers\\n\"\n",
        "\n",
        "        summary_content += f\"\"\"\n",
        "## Project Structure\n",
        "\n",
        "```\n",
        "{project_path.name}/\n",
        "├──  README.md (this file)\n",
        "├── data/\n",
        "│   ├── papers_data.json      # Raw paper metadata\n",
        "│   └── all_summaries.json    # Consolidated summaries\n",
        "├── summaries/                # Individual paper analyses\n",
        "├── notes/\n",
        "│   └── literature_review.md  # Generated literature review\n",
        "├── insights/\n",
        "│   └── research_insights.md  # Research trends and patterns\n",
        "└── papers/                   # Additional paper files\n",
        "```\n",
        "\n",
        "## Papers Analyzed\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "        for i, paper in enumerate(papers, 1):\n",
        "            authors_str = ', '.join(paper.authors[:3])\n",
        "            if len(paper.authors) > 3:\n",
        "                authors_str += f\" et al. ({len(paper.authors)} total)\"\n",
        "\n",
        "            summary_content += f\"\"\"### {i}. {paper.title}\n",
        "\n",
        "- **Authors:** {authors_str}\n",
        "- **Source:** {paper.source}\n",
        "- **Published:** {paper.published_date}\n",
        "- **URL:** [{paper.url}]({paper.url})\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "        summary_content += f\"\"\"\n",
        "## Next Steps\n",
        "\n",
        "1. **Review Literature Review** - Check `notes/literature_review.md` for comprehensive analysis\n",
        "2. **Explore Research Insights** - Browse `insights/research_insights.md` for trends and patterns\n",
        "3. **Examine Individual Summaries** - Browse the `summaries/` folder for detailed paper analyses\n",
        "4. **Download Results** - Use the download function to save your research locally\n",
        "5. **Extend Research** - Add more papers or create additional analyses\n",
        "\n",
        "##  How to Download\n",
        "\n",
        "Run the following code to download your complete research project:\n",
        "\n",
        "```python\n",
        "research_assistant.download_project(\"{project_path.name}\")\n",
        "```\n",
        "\n",
        "## Powered By\n",
        "\n",
        "- **AI Model:** Google Gemini (Free API)\n",
        "- **Paper Sources:** ArXiv, Semantic Scholar, PubMed\n",
        "- **Environment:** Google Colab\n",
        "\n",
        "---\n",
        "*Generated by Personal Research Assistant for Google Colab*\n",
        "\"\"\"\n",
        "\n",
        "        with open(project_path / \"README.md\", 'w', encoding='utf-8') as f:\n",
        "            f.write(summary_content)\n",
        "\n",
        "        print(\"Project summary created\")\n",
        "\n",
        "    def create_downloadable_zip(self, project_path: Path) -> str:\n",
        "        \"\"\"Create a downloadable zip file of the project\"\"\"\n",
        "        zip_path = f\"/content/{project_path.name}.zip\"\n",
        "\n",
        "        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "            for file_path in project_path.rglob('*'):\n",
        "                if file_path.is_file():\n",
        "                    arcname = file_path.relative_to(project_path.parent)\n",
        "                    zipf.write(file_path, arcname)\n",
        "\n",
        "        return zip_path\n",
        "\n",
        "# ========================================\n",
        "# MAIN RESEARCH ASSISTANT CLASS\n",
        "# ========================================\n",
        "\n",
        "class ColabResearchAssistant:\n",
        "    \"\"\"Main research assistant optimized for Google Colab with Gemini API\"\"\"\n",
        "\n",
        "    def __init__(self, gemini_api_key: str, model_name: str = \"gemini-1.5-flash\"):\n",
        "        self.fetcher = PaperFetcher()\n",
        "        self.summarizer = GeminiResearchSummarizer(gemini_api_key, model_name)\n",
        "        self.organizer = ColabResearchOrganizer()\n",
        "        self.current_project_path = None\n",
        "\n",
        "    def conduct_research(self, query: str, max_papers_per_source: int = 8,\n",
        "                        project_name: Optional[str] = None,\n",
        "                        sources: List[str] = [\"arxiv\", \"semantic_scholar\", \"pubmed\"]) -> Path:\n",
        "        \"\"\"Conduct complete research process with progress indicators\"\"\"\n",
        "\n",
        "        if not project_name:\n",
        "            project_name = f\"{query.replace(' ', '_')[:30]}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "\n",
        "        print(\"STARTING RESEARCH PROJECT\")\n",
        "        print(\"=\" * 50)\n",
        "        print(f\"Project: {project_name}\")\n",
        "        print(f\" Query: '{query}'\")\n",
        "        print(f\"Max papers per source: {max_papers_per_source}\")\n",
        "        print(f\"Sources: {', '.join(sources)}\")\n",
        "        print(f\"AI Model: Google Gemini\")\n",
        "        print()\n",
        "\n",
        "        # Create project folder\n",
        "        project_path = self.organizer.create_project_folder(project_name)\n",
        "        self.current_project_path = project_path\n",
        "        print(f\"Project folder created: {project_path}\")\n",
        "        print()\n",
        "\n",
        "        # Fetch papers from selected sources\n",
        "        all_papers = []\n",
        "\n",
        "        if \"arxiv\" in sources:\n",
        "            arxiv_papers = self.fetcher.fetch_arxiv_papers(query, max_papers_per_source)\n",
        "            all_papers.extend(arxiv_papers)\n",
        "\n",
        "        if \"semantic_scholar\" in sources:\n",
        "            ss_papers = self.fetcher.fetch_semantic_scholar_papers(query, max_papers_per_source)\n",
        "            all_papers.extend(ss_papers)\n",
        "\n",
        "        if \"pubmed\" in sources:\n",
        "            pubmed_papers = self.fetcher.fetch_pubmed_papers(query, max_papers_per_source)\n",
        "            all_papers.extend(pubmed_papers)\n",
        "\n",
        "        if not all_papers:\n",
        "            print(\"No papers found for the given query\")\n",
        "            return project_path\n",
        "\n",
        "        print(f\"\\n PAPER COLLECTION SUMMARY\")\n",
        "        print(\"-\" * 30)\n",
        "        print(f\"Total papers found: {len(all_papers)}\")\n",
        "\n",
        "        # Remove duplicates\n",
        "        unique_papers = self.remove_duplicate_papers(all_papers)\n",
        "        print(f\"Unique papers after deduplication: {len(unique_papers)}\")\n",
        "        print()\n",
        "\n",
        "        # Save paper data\n",
        "        self.organizer.save_paper_data(project_path, unique_papers)\n",
        "\n",
        "        # Generate summaries with progress tracking\n",
        "        print(\"GENERATING AI SUMMARIES WITH GEMINI\")\n",
        "        print(\"-\" * 40)\n",
        "        summaries = []\n",
        "        for i, paper in enumerate(unique_papers, 1):\n",
        "            print(f\" Processing {i}/{len(unique_papers)}: {paper.title[:50]}...\")\n",
        "            summary = self.summarizer.summarize_paper(paper)\n",
        "            summaries.append(summary)\n",
        "\n",
        "            # Rate limiting for Gemini API (more generous than OpenAI)\n",
        "            if i < len(unique_papers):\n",
        "                time.sleep(1)  # Reduced delay\n",
        "\n",
        "        print(f\"Generated {len(summaries)} paper summaries\")\n",
        "        print()\n",
        "\n",
        "        # Save summaries\n",
        "        self.organizer.save_summaries(project_path, unique_papers, summaries)\n",
        "\n",
        "        # Generate literature review\n",
        "        print(\"GENERATING LITERATURE REVIEW\")\n",
        "        print(\"-\" * 30)\n",
        "        literature_review = self.summarizer.generate_literature_review(unique_papers, summaries, query)\n",
        "        self.organizer.save_literature_review(project_path, literature_review, query)\n",
        "        print(\" Literature review generated\")\n",
        "        print()\n",
        "\n",
        "        # Generate research insights\n",
        "        print(\"GENERATING RESEARCH INSIGHTS\")\n",
        "        print(\"-\" * 30)\n",
        "        insights = self.summarizer.generate_research_insights(unique_papers, summaries, query)\n",
        "        self.organizer.save_research_insights(project_path, insights, query)\n",
        "        print(\"Research insights generated\")\n",
        "        print()\n",
        "\n",
        "        # Create project summary\n",
        "        self.organizer.create_project_summary(project_path, query, unique_papers)\n",
        "\n",
        "        print(\"RESEARCH PROJECT COMPLETED!\")\n",
        "        print(\"=\" * 50)\n",
        "        print(f\"Location: {project_path}\")\n",
        "        print(f\"Papers analyzed: {len(unique_papers)}\")\n",
        "        print(f\"Files generated: {len(list(project_path.rglob('*')))} files\")\n",
        "        print(f\"Powered by: Google Gemini API\")\n",
        "        print()\n",
        "        print(\"To explore your results:\")\n",
        "        print(f\"   • Project overview: {project_path}/README.md\")\n",
        "        print(f\"   • Literature review: {project_path}/notes/literature_review.md\")\n",
        "        print(f\"   • Research insights: {project_path}/insights/research_insights.md\")\n",
        "        print(f\"   • Paper summaries: {project_path}/summaries/\")\n",
        "        print()\n",
        "\n",
        "        return project_path\n",
        "\n",
        "    def remove_duplicate_papers(self, papers: List[Paper]) -> List[Paper]:\n",
        "        \"\"\"Remove duplicate papers based on title similarity\"\"\"\n",
        "        unique_papers = []\n",
        "        seen_titles = set()\n",
        "\n",
        "        for paper in papers:\n",
        "            normalized_title = paper.title.lower().strip().replace(' ', '')\n",
        "            if normalized_title not in seen_titles:\n",
        "                seen_titles.add(normalized_title)\n",
        "                unique_papers.append(paper)\n",
        "\n",
        "        return unique_papers\n",
        "\n",
        "    def display_project_summary(self, project_path: Path = None):\n",
        "        \"\"\"Display a nice summary of the research project\"\"\"\n",
        "        if project_path is None:\n",
        "            project_path = self.current_project_path\n",
        "\n",
        "        if not project_path or not project_path.exists():\n",
        "            print(\"No project found to display\")\n",
        "            return\n",
        "\n",
        "        # Read project data\n",
        "        papers_file = project_path / \"data\" / \"papers_data.json\"\n",
        "        if papers_file.exists():\n",
        "            with open(papers_file, 'r') as f:\n",
        "                papers_data = json.load(f)\n",
        "\n",
        "            print(\"PROJECT SUMMARY\")\n",
        "            print(\"=\" * 40)\n",
        "            print(f\" Project: {project_path.name}\")\n",
        "            print(f\" Papers: {len(papers_data)}\")\n",
        "\n",
        "            # Source breakdown\n",
        "            sources = {}\n",
        "            for paper in papers_data:\n",
        "                source = paper['source']\n",
        "                sources[source] = sources.get(source, 0) + 1\n",
        "\n",
        "            print(\"\\n Sources:\")\n",
        "            for source, count in sources.items():\n",
        "                print(f\"   • {source}: {count} papers\")\n",
        "\n",
        "            print(f\"\\n Files created: {len(list(project_path.rglob('*')))} files\")\n",
        "            print(f\" Total size: {self.get_folder_size(project_path):.2f} MB\")\n",
        "            print(f\" AI Model: Google Gemini\")\n",
        "\n",
        "    def get_folder_size(self, folder_path: Path) -> float:\n",
        "        \"\"\"Calculate folder size in MB\"\"\"\n",
        "        total_size = 0\n",
        "        for file_path in folder_path.rglob('*'):\n",
        "            if file_path.is_file():\n",
        "                total_size += file_path.stat().st_size\n",
        "        return total_size / (1024 * 1024)\n",
        "\n",
        "    def download_project(self, project_name: str = None):\n",
        "        \"\"\"Create and download a zip file of the research project\"\"\"\n",
        "        if project_name:\n",
        "            project_path = self.organizer.base_path / project_name\n",
        "        else:\n",
        "            project_path = self.current_project_path\n",
        "\n",
        "        if not project_path or not project_path.exists():\n",
        "            print(\"Project not found\")\n",
        "            return\n",
        "\n",
        "        print(f\"Creating download package for: {project_path.name}\")\n",
        "        zip_path = self.organizer.create_downloadable_zip(project_path)\n",
        "\n",
        "        print(f\"Download package ready!\")\n",
        "        print(f\" Size: {Path(zip_path).stat().st_size / (1024*1024):.2f} MB\")\n",
        "\n",
        "        # Download the file\n",
        "        files.download(zip_path)\n",
        "        print(\" Download started! Check your browser's downloads folder.\")\n",
        "\n",
        "    def quick_search(self, query: str, source: str = \"arxiv\", max_results: int = 5) -> List[Dict]:\n",
        "        \"\"\"Quick search function for immediate results without full processing\"\"\"\n",
        "        print(f\"Quick search: '{query}' from {source}\")\n",
        "\n",
        "        papers = []\n",
        "        if source.lower() == \"arxiv\":\n",
        "            papers = self.fetcher.fetch_arxiv_papers(query, max_results)\n",
        "        elif source.lower() == \"semantic_scholar\":\n",
        "            papers = self.fetcher.fetch_semantic_scholar_papers(query, max_results)\n",
        "        elif source.lower() == \"pubmed\":\n",
        "            papers = self.fetcher.fetch_pubmed_papers(query, max_results)\n",
        "\n",
        "        # Return simplified data\n",
        "        results = []\n",
        "        for paper in papers:\n",
        "            results.append({\n",
        "                \"title\": paper.title,\n",
        "                \"authors\": \", \".join(paper.authors[:3]),\n",
        "                \"source\": paper.source,\n",
        "                \"url\": paper.url,\n",
        "                \"abstract\": paper.abstract[:200] + \"...\" if len(paper.abstract) > 200 else paper.abstract\n",
        "            })\n",
        "\n",
        "        return results\n",
        "\n",
        "    def list_projects(self):\n",
        "        \"\"\"List all existing research projects\"\"\"\n",
        "        projects = [d for d in self.organizer.base_path.iterdir() if d.is_dir()]\n",
        "\n",
        "        if not projects:\n",
        "            print(\"No research projects found.\")\n",
        "            return\n",
        "\n",
        "        print(\"EXISTING RESEARCH PROJECTS\")\n",
        "        print(\"=\" * 40)\n",
        "\n",
        "        for i, project in enumerate(sorted(projects), 1):\n",
        "            # Try to read project info\n",
        "            readme_path = project / \"README.md\"\n",
        "            papers_path = project / \"data\" / \"papers_data.json\"\n",
        "\n",
        "            paper_count = 0\n",
        "            if papers_path.exists():\n",
        "                try:\n",
        "                    with open(papers_path, 'r') as f:\n",
        "                        paper_count = len(json.load(f))\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "            size_mb = self.get_folder_size(project)\n",
        "            print(f\"{i}. {project.name}\")\n",
        "            print(f\"   Papers: {paper_count} | 💾 Size: {size_mb:.1f}MB\")\n",
        "            print(f\"   Path: {project}\")\n",
        "            print()\n",
        "\n",
        "# ========================================\n",
        "# EASY-TO-USE FUNCTIONS FOR COLAB\n",
        "# ========================================\n",
        "\n",
        "def quick_research(query: str, max_papers: int = 10, model: str = \"gemini-1.5-flash\"):\n",
        "    \"\"\"\n",
        "    Quick research function - just provide a query!\n",
        "\n",
        "    Args:\n",
        "        query (str): Your research question or topic\n",
        "        max_papers (int): Maximum papers per source (default: 10)\n",
        "        model (str): Gemini model to use (default: gemini-1.5-flash for speed)\n",
        "\n",
        "    Returns:\n",
        "        ColabResearchAssistant: The research assistant instance\n",
        "    \"\"\"\n",
        "\n",
        "    # Setup configuration\n",
        "    config = ColabConfig()\n",
        "    api_key = config.setup_gemini_key()\n",
        "\n",
        "    # Initialize research assistant\n",
        "    assistant = ColabResearchAssistant(api_key, model)\n",
        "\n",
        "    # Conduct research\n",
        "    project_path = assistant.conduct_research(\n",
        "        query=query,\n",
        "        max_papers_per_source=max_papers,\n",
        "        sources=[\"arxiv\", \"semantic_scholar\", \"pubmed\"]\n",
        "    )\n",
        "\n",
        "    # Display summary\n",
        "    assistant.display_project_summary()\n",
        "\n",
        "    return assistant\n",
        "\n",
        "def setup_research_environment():\n",
        "    \"\"\"Setup the research environment and return a configured assistant\"\"\"\n",
        "    print(\"🔧 SETTING UP RESEARCH ENVIRONMENT\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "    config = ColabConfig()\n",
        "    api_key = config.setup_gemini_key()\n",
        "\n",
        "    assistant = ColabResearchAssistant(api_key)\n",
        "\n",
        "    print(\"Environment ready!\")\n",
        "    print(\"\\n🚀 Quick start:\")\n",
        "    print(\"assistant.conduct_research('your research topic')\")\n",
        "    print(\"\\n📚 Or use the quick function:\")\n",
        "    print(\"quick_research('your research topic')\")\n",
        "\n",
        "    return assistant\n",
        "\n",
        "def quick_search(query: str, source: str = \"arxiv\", max_results: int = 5):\n",
        "    \"\"\"\n",
        "    Quick search without full processing - just get paper titles and abstracts\n",
        "\n",
        "    Args:\n",
        "        query (str): Search query\n",
        "        source (str): \"arxiv\", \"semantic_scholar\", or \"pubmed\"\n",
        "        max_results (int): Number of results\n",
        "    \"\"\"\n",
        "    config = ColabConfig()\n",
        "    api_key = config.setup_gemini_key()\n",
        "\n",
        "    assistant = ColabResearchAssistant(api_key)\n",
        "    results = assistant.quick_search(query, source, max_results)\n",
        "\n",
        "    print(f\"🔍 Quick Search Results: '{query}' from {source}\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    for i, paper in enumerate(results, 1):\n",
        "        print(f\"{i}. **{paper['title']}**\")\n",
        "        print(f\"   Authors: {paper['authors']}\")\n",
        "        print(f\"   Source: {paper['source']}\")\n",
        "        print(f\"   Abstract: {paper['abstract']}\")\n",
        "        print(f\"   URL: {paper['url']}\")\n",
        "        print()\n",
        "\n",
        "    return results\n",
        "\n",
        "# ========================================\n",
        "# EXAMPLE USAGE AND INSTRUCTIONS\n",
        "# ========================================\n",
        "\n",
        "def show_examples():\n",
        "    \"\"\"Show example usage patterns\"\"\"\n",
        "    print(\"\"\"\n",
        "🔬 PERSONAL RESEARCH ASSISTANT - GEMINI VERSION\n",
        "===============================================\n",
        "\n",
        "🆓 **FREE Google Gemini API** - Get your key at: https://aistudio.google.com/app/apikey\n",
        "\n",
        "## 🚀 QUICK START EXAMPLES\n",
        "\n",
        "1. **INSTANT RESEARCH** (Easiest method):\n",
        "   ```python\n",
        "   assistant = quick_research(\"machine learning in healthcare\", max_papers=15)\n",
        "   ```\n",
        "\n",
        "2. **SETUP ONCE, USE MULTIPLE TIMES**:\n",
        "   ```python\n",
        "   assistant = setup_research_environment()\n",
        "   assistant.conduct_research(\"quantum computing algorithms\", max_papers_per_source=12)\n",
        "   ```\n",
        "\n",
        "3. **QUICK PAPER SEARCH** (No AI processing):\n",
        "   ```python\n",
        "   results = quick_search(\"neural networks\", source=\"arxiv\", max_results=10)\n",
        "   ```\n",
        "\n",
        "4. **CUSTOM RESEARCH WITH MULTIPLE SOURCES**:\n",
        "   ```python\n",
        "   assistant.conduct_research(\n",
        "       query=\"natural language processing transformers\",\n",
        "       max_papers_per_source=20,\n",
        "       project_name=\"NLP_Transformers_2024\",\n",
        "       sources=[\"arxiv\", \"semantic_scholar\", \"pubmed\"]\n",
        "   )\n",
        "   ```\n",
        "\n",
        "## 📥 DOWNLOAD & MANAGE\n",
        "\n",
        "```python\n",
        "# Download current project\n",
        "assistant.download_project()\n",
        "\n",
        "# Download specific project\n",
        "assistant.download_project(\"specific_project_name\")\n",
        "\n",
        "# List all projects\n",
        "assistant.list_projects()\n",
        "```\n",
        "\n",
        "## 🆕 NEW FEATURES\n",
        "\n",
        "✅ **Google Gemini AI** - Free API with generous limits\n",
        "✅ **PubMed Integration** - Medical/biological research papers\n",
        "✅ **Research Insights** - AI-generated trends and patterns analysis\n",
        "✅ **Quick Search** - Fast paper discovery without full processing\n",
        "✅ **Project Management** - List, download, and organize multiple projects\n",
        "\n",
        "## 📊 SOURCES AVAILABLE\n",
        "\n",
        "- **ArXiv** - Physics, Math, Computer Science, Biology\n",
        "- **Semantic Scholar** - Multidisciplinary academic papers\n",
        "- **PubMed** - Medical and life sciences literature\n",
        "\n",
        "## 💡 PRO TIPS\n",
        "\n",
        "• **Start small**: Begin with 8-12 papers per source for faster results\n",
        "• **Use specific keywords**: \"machine learning healthcare\" vs \"AI medicine\"\n",
        "• **Try different sources**: Each has different paper collections\n",
        "• **Download results**: Save your work locally for future reference\n",
        "• **Check insights**: New AI-generated research trends analysis\n",
        "\n",
        "## 🔧 MODELS AVAILABLE\n",
        "\n",
        "- **gemini-1.5-flash** (default) - Fast and efficient\n",
        "- **gemini-1.5-pro** - More detailed analysis (slower)\n",
        "\n",
        "## 📈 COST COMPARISON\n",
        "\n",
        "**Google Gemini FREE**: 15 requests/minute, 1500 requests/day\n",
        "**OpenAI GPT**: $0.002-0.06 per 1K tokens (paid only)\n",
        "\n",
        "## 🚨 GETTING STARTED\n",
        "\n",
        "```python\n",
        "# Just run this and follow the prompts!\n",
        "quick_research(\"your research topic here\")\n",
        "```\n",
        "\n",
        "Happy researching! 🎓\n",
        "    \"\"\")\n",
        "\n",
        "def show_gemini_setup():\n",
        "    \"\"\"Show detailed Gemini API setup instructions\"\"\"\n",
        "    print(\"\"\"\n",
        "🔑 GOOGLE GEMINI API SETUP GUIDE\n",
        "================================\n",
        "\n",
        "## Step 1: Get Your Free API Key\n",
        "1. Go to: https://aistudio.google.com/app/apikey\n",
        "2. Sign in with your Google account\n",
        "3. Click \"Create API Key\"\n",
        "4. Copy your API key (starts with 'AI...')\n",
        "\n",
        "## Step 2: Run the Setup\n",
        "```python\n",
        "assistant = setup_research_environment()\n",
        "```\n",
        "\n",
        "## Step 3: Enter Your Key\n",
        "- Paste your API key when prompted\n",
        "- It will be securely stored for this session\n",
        "\n",
        "## ✅ You're Ready!\n",
        "```python\n",
        "quick_research(\"your topic\")\n",
        "```\n",
        "\n",
        "## 🆓 Free Limits (Very Generous!)\n",
        "- 15 requests per minute\n",
        "- 1,500 requests per day\n",
        "- No credit card required\n",
        "\n",
        "## 🔒 Privacy\n",
        "- API keys are stored only in your Colab session\n",
        "- Not saved permanently\n",
        "- Google's standard privacy policies apply\n",
        "    \"\"\")\n",
        "\n",
        "# Auto-display examples when notebook loads\n",
        "print(\"🎯 Personal Research Assistant - Gemini Version Ready!\")\n",
        "print(\"🆓 Powered by Google's FREE Gemini API\")\n",
        "print()\n",
        "print(\"📚 Type: show_examples() to see usage examples\")\n",
        "print(\"🔑 Type: show_gemini_setup() for API setup help\")\n",
        "print(\"🚀 Type: quick_research('your topic') to start immediately\")\n",
        "print()\n",
        "print(\"Get your FREE Gemini API key: https://aistudio.google.com/app/apikey\")"
      ],
      "metadata": {
        "id": "nMzAGHjt9Izm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7e3742f-3322-43b2-a6c6-a234f2680a02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎯 Personal Research Assistant - Gemini Version Ready!\n",
            "🆓 Powered by Google's FREE Gemini API\n",
            "\n",
            "📚 Type: show_examples() to see usage examples\n",
            "🔑 Type: show_gemini_setup() for API setup help\n",
            "🚀 Type: quick_research('your topic') to start immediately\n",
            "\n",
            "Get your FREE Gemini API key: https://aistudio.google.com/app/apikey\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the research assistant code above first, then use:\n",
        "\n",
        "# Method 1: Super quick research\n",
        "assistant = quick_research(\"machine learning in healthcare\", max_papers=10)\n",
        "\n",
        "# Method 2: Step-by-step setup\n",
        "assistant = setup_research_environment()\n",
        "assistant.conduct_research(\"CNN\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "njdLjsKx8RO8",
        "outputId": "10acea7e-edf8-469d-973b-cab28452c93c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔑 Google Gemini API Key Setup\n",
            "========================================\n",
            "Get your FREE API key at: https://aistudio.google.com/app/apikey\n",
            "\n",
            "Enter your Google Gemini API key (hidden input): ··········\n",
            "✅ API key configured successfully!\n",
            "🚀 STARTING RESEARCH PROJECT\n",
            "==================================================\n",
            "📝 Project: machine_learning_in_healthcare_20250808_091022\n",
            "🔍 Query: 'machine learning in healthcare'\n",
            "📊 Max papers per source: 10\n",
            "🌐 Sources: arxiv, semantic_scholar, pubmed\n",
            "🤖 AI Model: Google Gemini\n",
            "\n",
            "📁 Project folder created: /content/research_projects/machine_learning_in_healthcare_20250808_091022\n",
            "\n",
            "🔍 Searching ArXiv for: 'machine learning in healthcare'\n",
            "✅ Found 10 papers from ArXiv\n",
            "🔍 Searching Semantic Scholar for: 'machine learning in healthcare'\n",
            "✅ Found 5 papers from Semantic Scholar\n",
            "🔍 Searching PubMed for: 'machine learning in healthcare'\n",
            "✅ Found 10 papers from PubMed\n",
            "\n",
            "📊 PAPER COLLECTION SUMMARY\n",
            "------------------------------\n",
            "Total papers found: 25\n",
            "Unique papers after deduplication: 25\n",
            "\n",
            "💾 Saved 25 papers to papers_data.json\n",
            "🤖 GENERATING AI SUMMARIES WITH GEMINI\n",
            "----------------------------------------\n",
            "📄 Processing 1/25: FaceAnonyMixer: Cancelable Faces via Identity Cons...\n",
            "📄 Summarizing: FaceAnonyMixer: Cancelable Faces via Identity Consistent Lat...\n",
            "📄 Processing 2/25: Genie Envisioner: A Unified World Foundation Platf...\n",
            "📄 Summarizing: Genie Envisioner: A Unified World Foundation Platform for Ro...\n",
            "📄 Processing 3/25: Towards Generalizable Safety in Crowd Navigation v...\n",
            "📄 Summarizing: Towards Generalizable Safety in Crowd Navigation via Conform...\n",
            "📄 Processing 4/25: KuaiLive: A Real-time Interactive Dataset for Live...\n",
            "📄 Summarizing: KuaiLive: A Real-time Interactive Dataset for Live Streaming...\n",
            "📄 Processing 5/25: Partial projected ensembles and spatiotemporal str...\n",
            "📄 Summarizing: Partial projected ensembles and spatiotemporal structure of ...\n",
            "📄 Processing 6/25: MOSEv2: A More Challenging Dataset for Video Objec...\n",
            "📄 Summarizing: MOSEv2: A More Challenging Dataset for Video Object Segmenta...\n",
            "📄 Processing 7/25: GAP: Gaussianize Any Point Clouds with Text Guidan...\n",
            "📄 Summarizing: GAP: Gaussianize Any Point Clouds with Text Guidance...\n",
            "📄 Processing 8/25: On the Generalization of SFT: A Reinforcement Lear...\n",
            "📄 Summarizing: On the Generalization of SFT: A Reinforcement Learning Persp...\n",
            "📄 Processing 9/25: H-Net++: Hierarchical Dynamic Chunking for Tokeniz...\n",
            "📄 Summarizing: H-Net++: Hierarchical Dynamic Chunking for Tokenizer-Free La...\n",
            "📄 Processing 10/25: LiDO: Discovery of a 10:1 Resonator with a Novel L...\n",
            "📄 Summarizing: LiDO: Discovery of a 10:1 Resonator with a Novel Libration S...\n",
            "📄 Processing 11/25: A Comprehensive Review on Machine Learning in Heal...\n",
            "📄 Summarizing: A Comprehensive Review on Machine Learning in Healthcare Ind...\n",
            "📄 Processing 12/25: Quantum Machine Learning in Healthcare: Developmen...\n",
            "📄 Summarizing: Quantum Machine Learning in Healthcare: Developments and Cha...\n",
            "⚠️ Error summarizing paper: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "📄 Processing 13/25: Combining simulation models and machine learning i...\n",
            "📄 Summarizing: Combining simulation models and machine learning in healthca...\n",
            "📄 Processing 14/25: Interpretable Machine Learning in Healthcare...\n",
            "📄 Summarizing: Interpretable Machine Learning in Healthcare...\n",
            "⚠️ Error summarizing paper: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "📄 Processing 15/25: Machine Learning in Healthcare: A Review...\n",
            "📄 Summarizing: Machine Learning in Healthcare: A Review...\n",
            "📄 Processing 16/25: Interpretable machine learning-based prediction of...\n",
            "📄 Summarizing: Interpretable machine learning-based prediction of mortality...\n",
            "📄 Processing 17/25: Influencing factors and dynamic changes of COVID-1...\n",
            "📄 Summarizing: Influencing factors and dynamic changes of COVID-19 vaccine ...\n",
            "📄 Processing 18/25: AlphaBind, a domain-specific model to predict and ...\n",
            "📄 Summarizing: AlphaBind, a domain-specific model to predict and optimize a...\n",
            "📄 Processing 19/25: AI-driven pharmacovigilance: Enhancing adverse dru...\n",
            "📄 Summarizing: AI-driven pharmacovigilance: Enhancing adverse drug reaction...\n",
            "📄 Processing 20/25: Projecting lyme disease risk in the United States:...\n",
            "📄 Summarizing: Projecting lyme disease risk in the United States: A machine...\n",
            "📄 Processing 21/25: CnnBoost: a multilevel explainable stacked ensembl...\n",
            "📄 Summarizing: CnnBoost: a multilevel explainable stacked ensemble framewor...\n",
            "📄 Processing 22/25: Adverse impact of paternal age on embryo euploidy:...\n",
            "📄 Summarizing: Adverse impact of paternal age on embryo euploidy: insights ...\n",
            "📄 Processing 23/25: Integrating artificial intelligence in healthcare:...\n",
            "📄 Summarizing: Integrating artificial intelligence in healthcare: applicati...\n",
            "📄 Processing 24/25: High-pitch photon-counting detector computed tomog...\n",
            "📄 Summarizing: High-pitch photon-counting detector computed tomography angi...\n",
            "📄 Processing 25/25: Development and validation of a hierarchical machi...\n",
            "📄 Summarizing: Development and validation of a hierarchical machine learnin...\n",
            "✅ Generated 25 paper summaries\n",
            "\n",
            "💾 Saved 25 summaries\n",
            "📚 GENERATING LITERATURE REVIEW\n",
            "------------------------------\n",
            "📚 Generating literature review...\n",
            "💾 Literature review saved\n",
            "✅ Literature review generated\n",
            "\n",
            "🔍 GENERATING RESEARCH INSIGHTS\n",
            "------------------------------\n",
            "🔍 Generating research insights...\n",
            "💾 Research insights saved\n",
            "✅ Research insights generated\n",
            "\n",
            "💾 Project summary created\n",
            "🎉 RESEARCH PROJECT COMPLETED!\n",
            "==================================================\n",
            "📁 Location: /content/research_projects/machine_learning_in_healthcare_20250808_091022\n",
            "📊 Papers analyzed: 25\n",
            "📄 Files generated: 35 files\n",
            "🤖 Powered by: Google Gemini API\n",
            "\n",
            "🔍 To explore your results:\n",
            "   • Project overview: /content/research_projects/machine_learning_in_healthcare_20250808_091022/README.md\n",
            "   • Literature review: /content/research_projects/machine_learning_in_healthcare_20250808_091022/notes/literature_review.md\n",
            "   • Research insights: /content/research_projects/machine_learning_in_healthcare_20250808_091022/insights/research_insights.md\n",
            "   • Paper summaries: /content/research_projects/machine_learning_in_healthcare_20250808_091022/summaries/\n",
            "\n",
            "📊 PROJECT SUMMARY\n",
            "========================================\n",
            "📁 Project: machine_learning_in_healthcare_20250808_091022\n",
            "📄 Papers: 25\n",
            "\n",
            "🌐 Sources:\n",
            "   • ArXiv: 10 papers\n",
            "   • Semantic Scholar: 5 papers\n",
            "   • PubMed: 10 papers\n",
            "\n",
            "📂 Files created: 35 files\n",
            "💾 Total size: 0.18 MB\n",
            "🤖 AI Model: Google Gemini\n",
            "🔧 SETTING UP RESEARCH ENVIRONMENT\n",
            "========================================\n",
            "🔑 Google Gemini API Key Setup\n",
            "========================================\n",
            "Get your FREE API key at: https://aistudio.google.com/app/apikey\n",
            "\n",
            "✅ Gemini API key already configured!\n",
            "✅ Environment ready!\n",
            "\n",
            "🚀 Quick start:\n",
            "assistant.conduct_research('your research topic')\n",
            "\n",
            "📚 Or use the quick function:\n",
            "quick_research('your research topic')\n",
            "🚀 STARTING RESEARCH PROJECT\n",
            "==================================================\n",
            "📝 Project: CNN_20250808_091236\n",
            "🔍 Query: 'CNN'\n",
            "📊 Max papers per source: 8\n",
            "🌐 Sources: arxiv, semantic_scholar, pubmed\n",
            "🤖 AI Model: Google Gemini\n",
            "\n",
            "📁 Project folder created: /content/research_projects/CNN_20250808_091236\n",
            "\n",
            "🔍 Searching ArXiv for: 'CNN'\n",
            "✅ Found 8 papers from ArXiv\n",
            "🔍 Searching Semantic Scholar for: 'CNN'\n",
            "✅ Found 8 papers from Semantic Scholar\n",
            "🔍 Searching PubMed for: 'CNN'\n",
            "✅ Found 8 papers from PubMed\n",
            "\n",
            "📊 PAPER COLLECTION SUMMARY\n",
            "------------------------------\n",
            "Total papers found: 24\n",
            "Unique papers after deduplication: 24\n",
            "\n",
            "💾 Saved 24 papers to papers_data.json\n",
            "🤖 GENERATING AI SUMMARIES WITH GEMINI\n",
            "----------------------------------------\n",
            "📄 Processing 1/24: MM2CT: MR-to-CT translation for multi-modal image ...\n",
            "📄 Summarizing: MM2CT: MR-to-CT translation for multi-modal image fusion wit...\n",
            "📄 Processing 2/24: Advancing Hate Speech Detection with Transformers:...\n",
            "📄 Summarizing: Advancing Hate Speech Detection with Transformers: Insights ...\n",
            "📄 Processing 3/24: A deep learning approach to track eye movements ba...\n",
            "📄 Summarizing: A deep learning approach to track eye movements based on eve...\n",
            "⚠️ Error summarizing paper: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "📄 Processing 4/24: Advanced Multi-Architecture Deep Learning Framewor...\n",
            "📄 Summarizing: Advanced Multi-Architecture Deep Learning Framework for BIRA...\n",
            "📄 Processing 5/24: How Does Bilateral Ear Symmetry Affect Deep Ear Fe...\n",
            "📄 Summarizing: How Does Bilateral Ear Symmetry Affect Deep Ear Features?...\n",
            "📄 Processing 6/24: Share Your Attention: Transformer Weight Sharing v...\n",
            "📄 Summarizing: Share Your Attention: Transformer Weight Sharing via Matrix-...\n",
            "⚠️ Error summarizing paper: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "📄 Processing 7/24: LA-CaRe-CNN: Cascading Refinement CNN for Left Atr...\n",
            "📄 Summarizing: LA-CaRe-CNN: Cascading Refinement CNN for Left Atrial Scar S...\n",
            "📄 Processing 8/24: Bridging Simulation and Experiment: A Self-Supervi...\n",
            "📄 Summarizing: Bridging Simulation and Experiment: A Self-Supervised Domain...\n",
            "📄 Processing 9/24: Dynamic Graph CNN for Learning on Point Clouds...\n",
            "📄 Summarizing: Dynamic Graph CNN for Learning on Point Clouds...\n",
            "📄 Processing 10/24: Mask R-CNN...\n",
            "📄 Summarizing: Mask R-CNN...\n",
            "📄 Processing 11/24: Cascade R-CNN: Delving Into High Quality Object De...\n",
            "📄 Summarizing: Cascade R-CNN: Delving Into High Quality Object Detection...\n",
            "📄 Processing 12/24: Fast R-CNN...\n",
            "📄 Summarizing: Fast R-CNN...\n",
            "📄 Processing 13/24: Faster R-CNN: Towards Real-Time Object Detection w...\n",
            "📄 Summarizing: Faster R-CNN: Towards Real-Time Object Detection with Region...\n",
            "📄 Processing 14/24: Review of deep learning: concepts, CNN architectur...\n",
            "📄 Summarizing: Review of deep learning: concepts, CNN architectures, challe...\n",
            "📄 Processing 15/24: ShuffleNet V2: Practical Guidelines for Efficient ...\n",
            "📄 Summarizing: ShuffleNet V2: Practical Guidelines for Efficient CNN Archit...\n",
            "📄 Processing 16/24: Deep Convolutional Neural Networks for Computer-Ai...\n",
            "📄 Summarizing: Deep Convolutional Neural Networks for Computer-Aided Detect...\n",
            "📄 Processing 17/24: Quantitative Analysis of Deltamethrin Residues in ...\n",
            "📄 Summarizing: Quantitative Analysis of Deltamethrin Residues in Water Usin...\n",
            "⚠️ Error summarizing paper: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "📄 Processing 18/24: Micropore array-based SERS sensor assisted by conv...\n",
            "📄 Summarizing: Micropore array-based SERS sensor assisted by convolutional ...\n",
            "📄 Processing 19/24: Diagnosis of uterine diseases by label-free serum ...\n",
            "📄 Summarizing: Diagnosis of uterine diseases by label-free serum SERS finge...\n",
            "📄 Processing 20/24: Deep learning approach with ConvNeXt-SE-attn model...\n",
            "📄 Summarizing: Deep learning approach with ConvNeXt-SE-attn model for in vi...\n",
            "📄 Processing 21/24: Novel EEG-based diagnostic framework for Major Dep...\n",
            "📄 Summarizing: Novel EEG-based diagnostic framework for Major Depressive Di...\n",
            "📄 Processing 22/24: Enhanced SqueezeNet model for detecting IoT-Bot at...\n",
            "📄 Summarizing: Enhanced SqueezeNet model for detecting IoT-Bot attacks: A c...\n",
            "⚠️ Error summarizing paper: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
            "📄 Processing 23/24: Integrated deep learning for cardiovascular risk a...\n",
            "📄 Summarizing: Integrated deep learning for cardiovascular risk assessment ...\n",
            "📄 Processing 24/24: AI-driven pharmacovigilance: Enhancing adverse dru...\n",
            "📄 Summarizing: AI-driven pharmacovigilance: Enhancing adverse drug reaction...\n",
            "✅ Generated 24 paper summaries\n",
            "\n",
            "💾 Saved 24 summaries\n",
            "📚 GENERATING LITERATURE REVIEW\n",
            "------------------------------\n",
            "📚 Generating literature review...\n",
            "💾 Literature review saved\n",
            "✅ Literature review generated\n",
            "\n",
            "🔍 GENERATING RESEARCH INSIGHTS\n",
            "------------------------------\n",
            "🔍 Generating research insights...\n",
            "💾 Research insights saved\n",
            "✅ Research insights generated\n",
            "\n",
            "💾 Project summary created\n",
            "🎉 RESEARCH PROJECT COMPLETED!\n",
            "==================================================\n",
            "📁 Location: /content/research_projects/CNN_20250808_091236\n",
            "📊 Papers analyzed: 24\n",
            "📄 Files generated: 34 files\n",
            "🤖 Powered by: Google Gemini API\n",
            "\n",
            "🔍 To explore your results:\n",
            "   • Project overview: /content/research_projects/CNN_20250808_091236/README.md\n",
            "   • Literature review: /content/research_projects/CNN_20250808_091236/notes/literature_review.md\n",
            "   • Research insights: /content/research_projects/CNN_20250808_091236/insights/research_insights.md\n",
            "   • Paper summaries: /content/research_projects/CNN_20250808_091236/summaries/\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('/content/research_projects/CNN_20250808_091236')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jO-ne0Ji8ev3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}